---
title: "Compte-rendu TP Projets Maths"
format: 
  typst:
    toc: true
    section-numbering: "1.1 :"
    author: Tom Chauvel
    keep-typ: true

render-on-save: true

engine: jupyter
jupyter: python3
---

```{=typst}
#pagebreak()

= Partie 1 : D√©courverte Python

== Exercice 1 : D√©couverte Python
```

Dans cette partie, on va se familiariser avec numpy et python.

```{python}
import numpy as np
```
```{python}
print( 2+3 )
```

```{python}
print( 2**10 )
```

```{python}
print( np.sin(2 * np.pi) )
```

environ 0, python approxime tr√®s mal les nombres flottants

```{python}
print( np.exp(1j * np.pi) + 1 )
```

```{=typst}
$= cos( pi ) - 1 + i times sin( pi ) = 0$
, m√™me probl√®me
```

```{python}
A = np.matrix([[1,2],[3,4]])
B = np.matrix([[2,3],[4,5]])

print( A*B )
```

```{python}
A = np.matrix([[1,7],[4,2]])

print( np.linalg.det(A) )
```


```{=typst}
#pagebreak()
== Exercice 2 : Eigenvalues, eigenvectors
```

Ici on va chercher les valeurs et vecteurs propres de la matrice A.

```{python}
A =1/2*np.matrix([
  [np.sqrt(3)+1,-2],
  [1,np.sqrt(3)-1]
])

res = np.linalg.eig(A)

for i in range(len(res.eigenvalues)):
  print(f'Valeur  Propre {i} : {res.eigenvalues[i]}')
  print(f'Vecteur Propre {i} : {res.eigenvectors[i]}')
```

Maintenant calculons le produit $A^x \times v$

```{python}
A =1/2* np.matrix([[np.sqrt(3)+1,-2],[1,np.sqrt(3)-1]])

v = np.matrix([[1],[2]])

for i in range(1,14):
  print(f'A**{i}*V : {np.dot(A**i,v)}')
```

On remarque que $A^13 \times v = A \times v$, donc que l'on obtient un cycle qui se r√©p√®te toutes les 13 fois. Mais il y a aussi une √©tape interm√©diaire au milieu ou le r√©sulat est l'oppos√© de celui de d√©part : $A^7 \times v = -A \times v$


```{=typst}
Faisons la m√™me chose pour B et C.
```


```{python}
B =1/np.sqrt(2)* np.matrix([[np.sqrt(3)+1,-2],[1,np.sqrt(3)-1]])

v = np.matrix([[1],[2]])

for i in range(1,13):
  print(f'B**{i}*V : {np.dot(B**i,v)}')
```

On remarque aussi le cycle, sauf qu'ici les nombres sont multipli√©s par -8 toutes les 6 fois, (on le remarque pour n=6 et n=12).


```{python}
C =1/2/np.sqrt(2)* np.matrix([[np.sqrt(3)+1,-2],[1,np.sqrt(3)-1]])

v = np.matrix([[1],[2]])

for i in range(1,13):
  print(f'C**{i}*V : {np.dot(C**i,v)}')
```

De m√™me, On remarque le cycle, sauf qu'ici les nombres sont divis√©s par -8 toutes les 6 fois, (on le remarque pour n=6 et n=12).



```{=typst}
#pagebreak()
== Exercice 3 : Visualiser des functions
```

Dans cette partie on va visualiser des fonctions. Puis les analyser.

```{python}
import matplotlib.pyplot as plt

def plotfunc(f):
  x = np.arange(-2,2,0.001)
  y = f(x)
  plt.plot(x,y)
  plt.show()

```


```{python}
#| fig-cap: "Une fonction cubique, rien de plus banal"
plotfunc(lambda x : x**3)
```

```{python}
#| fig-cap: "x**2*np.sin(1/x)"

plotfunc(lambda x : x**2*np.sin(1/x))
```

au voisinage de z√©ro, la fonction √† des petites variations qui sont d√ª au sinus. La fonction semble √™tre impaire.

```{python}
#| fig-cap: "x**2*np.sin(1/(x**2))"
plotfunc(lambda x : x**2*np.sin(1/(x**2)))
```

de m√™me; au voisinnage de z√©ro, le sinus fait quelque chose de sp√©cial. On dirait qu'elle fait l'inverse d'un sinus cardinal. Sinon la fonction √† l'air d'√™tre born√© entre -1/3 et 1 et elle semble √™tre paire.

```{python}
#| fig-cap: "1/(1+np.exp(-x))"
plotfunc(lambda x : 1/(1+np.exp(-x)))
```

La fameuse fonction sigmo√Øde. Elle est beaucoup utilis√© en IA car elle est born√©e entre 0 et 1 et elle est super simple √† d√©riv√©, ce qui est g√©nial pour recalculer les poids des neuronnes. 

```{python}
#| fig-cap: "1/(1+np.exp(-10x))"
plotfunc(lambda x : 1/(1+np.exp(-10*x)))
```

Ici on a toujours une fonction sigmo√Øde, mais avec un coefficient plus grand, ce qui modifie √ßa forme et la valeur de sa pente.

```{python}
#| fig-cap: "1/(1+np.exp(-100x))"
plotfunc(lambda x : 1/(1+np.exp(-100*x)))
```

Pareil, mais avec une pente encore plus verticale.

```{python}
#| fig-cap: "(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))"
plotfunc(lambda x : (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))
```

De premier abord, on dirait une fonction sigmo√Øde, sauf que celle-ci est compris entre -1 et 1, ce qui rends cette fonction impaire. Elle est aussi un peu plus courb√©e que la sigmo√Øde √† coefficient √† 1.

```{python}
#| fig-cap: "(np.exp(100*x)-np.exp(-100*x))/(np.exp(100*x)+np.exp(-100*x)))"
plotfunc(lambda x : (np.exp(100*x)-np.exp(-100*x))/(np.exp(100*x)+np.exp(-100*x)))
```

```{=typst}
On prend $1/(1 + e^(-100 x))$, on l'allonge un peu pour qu'elle soit born√©e entre -1 et 1, et on a la m√™me courbe.
```

```{=typst}
#pagebreak()

== Exercice 4 : Alg√®bre et g√©ometrie

=== Points Align√©s

comment d√©termin√© si trois points sont align√©s ? C'est tr√®s simple, on calcule l'√©quation de droite d√©fini par deux points et on regarde si le troisi√®me points suit l'√©quation.

```

```{python}

def areAligned(v1,v2,v3):
  sub = v1-v2
  a = sub[1]/sub[0]
  b = v1[1] - a * v1[0]

  return a*v3[0] + b == v3[1]

v1 = np.array([1,2])
v2 = np.array([2,3])
v3 = np.array([3,4])

print( areAligned(v1,v2,v3) )

```

```{=typst}
#pagebreak()

=== Point dans un triangle
Comment d√©finir si un point est un triangle ? Une m√©thode qui existe est de calcul√© la somme des angles entre le point d ( qui ne d√©finit pas le triangle ) et les sommets du triangle. Si cette somme est √©gale √† 2pi oui 360¬∞, c'est ok. 

C'est ce que j'ai fait dans la m√©thode ci-dessous, sauf que j'ai d√ª arrondir le r√©sultat car les nombres flottants en informatique, c'est pas pr√©cis.
```

```{python}

def isInTriangle(p1,p2,p3,p4):
  # soustraction des points pour avoir les vecteurs, puis on calcule la norme du vecteur
  a1 = p1-p4
  n1 = np.linalg.norm(a1)
  a2 = p2-p4
  n2 = np.linalg.norm(a2)
  a3 = p3-p4
  n3 = np.linalg.norm(a3)

  # on calcule les cos avec le produit scalaire
  cos1 = np.dot(a1,a2)/n1/n2
  cos2 = np.dot(a2,a3)/n2/n3
  cos3 = np.dot(a3,a1)/n3/n1

  # on somme les angles et on les arrondis
  return np.round( np.arccos(cos1) + np.arccos(cos2) + np.arccos(cos3), 3 ) == np.round(2*np.pi,3)


p1 = np.array([0,2])
p2 = np.array([2,0])
p3 = np.array([0,0])
p4 = np.array([0.5,0.5])

print( isInTriangle(p1,p2,p3,p4) )

```

```{=typst}
#pagebreak()

=== Point dans un triangle rotat√©

C'est la m√™me chose, sauf qu'ici on a va rotater le triangle d'un angle pi/2 sur son barycentre.

Mise √† part la rotation avec la matrice, rien de bien m√©chant.

```
```{python}

def centreMasse(p1,p2,p3):
  # calcul du barycentre
  return (p1+p2+p3)/3

def rotate(p1,angle,c):
  # on d√©place le point pour que le barycentre soit √† l'origine
  p1p = p1 - c

  # matrice de rotation
  matrixrot = np.array([[np.cos(angle),np.sin(angle)],[-np.sin(angle),np.cos(angle)]])

  # on rotate et on r√©ajoute le barycentre
  return matrixrot.dot(p1p) + c

def isInTriangleRotate(p1,p2,p3,p4,angle):
  c = centreMasse(p1,p2,p3)

  p1r = rotate(p1,angle,c)
  p2r = rotate(p2,angle,c)
  p3r = rotate(p3,angle,c)

  return isInTriangle(p1r,p2r,p3r,p4)

p1 = np.array([0,2])
p2 = np.array([2,0])
p3 = np.array([0,0])
p4 = np.array([0.5,0.5])

print( isInTriangleRotate(p1,p2,p3,p4,-np.pi/2) )

```

```{=typst}
#pagebreak()

=== Point incident

Plus compliqu√© √† comprendre. On a un point, on veut savoir si ce point projet√© sur un plan, appartient au triangle qui d√©fini le plan ü§Ø.

Le plus dur, c'est le projet√© orthogonal, sinon c'est la m√™me m√©thode.

L'avantage, une formule existe :

Soit C un point du triangle, $accent(n, arrow)=vec(a,b,c)$ le vecteur normal du plan, et A le point √† projeter.

$
p(x) = a times x + b times y + c times z + d = 0
$

pour trouver d on prend un point du plan.

$
d = -( accent(n, arrow) dot C)
$

Pour le projet√© :

$
lambda = (a times X_A + b times Y_A + c times Z_A + d) / (a^2+b^2+c^2)
= ( accent(n, arrow) dot A) / (accent(n, arrow) dot accent(n, arrow))
$

$

A_p = vec(X_A - a times lambda,Y_A - b times lambda,Z_A - c times lambda)

$

Impl√©mentons cela :
```
```{python}

def isIncident(p1,p2,p3,p4,vn):
  d = - np.dot(vn,p1)
  lamb =  (np.dot(vn,p4)+d)/np.dot(vn,vn)
  ph = p4 - vn*lamb
  return isInTriangle(p1,p2,p3,ph)

p1 = np.array([4,2,-1])
p2 = np.array([1,3,1])
p3 = np.array([-3,0,3])
p4 = np.array([0,1.5,0])
vn = np.array([8,-2,13])

print( isIncident(p1,p2,p3,p4,vn) )

```

```{=typst}
#pagebreak()

=== Calcul de l'aire d'un triangle

D'un point de vue ext√©rieur c'est simple √† faire. Sauf que c'est long de calculer une hauteur sur des triangles qui ne sont pas toujours de la m√™me forme. Donc j'ai chercher un autre m√©thode, la formule d'H√©ron.

Soit p le demi-p√©rim√®tre, a b et c les c√¥t√©s du triangle

$
p = (a+b+c)/2
$$
S = sqrt( p(p-a)(p-b)(p-c) )
$
```

```{python}

def aireTriangle(p1,p2,p3):
  a = np.linalg.norm(p1-p2)
  b = np.linalg.norm(p1-p3)
  c = np.linalg.norm(p2-p3)

  p = (a+b+c)/2

  return (p*(p-a)*(p-b)*(p-c))**(1/2)

p1 = np.array([0,2])
p2 = np.array([2,0])
p3 = np.array([0,0])

print( aireTriangle(p1,p2,p3) )

```

```{=typst}
#pagebreak()

=== Polyg√¥ne convexe

Dernier algorithme, d√©terminer si un quadrilat√®re est convexe. Pour ce faire, on doit v√©rifier si tous les angles int√©rieurs sont inf√©rieurs √† 180¬∞. L'√©quivalent, c'est de faire √† chaque sommet, le produit vectorielle des arr√™tes adjacentes √† ce sommet, et v√©rifi√© que tous les produits vectorielle sont dans le m√™me sens, le produit vectorielle prend l'angle minimal entre les deux arr√™tes et l'oriente en fonction.

```

```{python}

def est_convexe(p1,p2,p3,p4):
  v1 = p2-p1
  v2 = p3-p2
  v3 = p4-p3
  v4 = p1-p4

  a = np.cross(v1,v2)
  b = np.cross(v2,v3)
  c = np.cross(v3,v4)
  d = np.cross(v4,v1)

  cas1 = a <= 0 and b <= 0 and c <= 0 and d <= 0 
  cas2 = a >= 0 and b >= 0 and c >= 0 and d >= 0 

  return cas1 or cas2

est_convexe( np.array([0, 0]), np.array([1, 1]), np.array([2, 0]), np.array([1, -1]) )

```

```{=typst}
#pagebreak()
= Partie 2 : Alg√®bre lin√©aire et apprentissage par la machine
== Exercice 1 : R√©solution de syst√®mes d‚Äô√©quations lin√©aires
On doit r√©soudre $A x = b$

Ce qui revient √† faire $x = A^(-1) b$
```


```{python}
import numpy as np
```

```{python}

A = np.matrix([
  [ 0, 2, 0, 1],
  [ 2, 2, 3, 2],
  [ 4,-3, 0, 1],
  [ 6, 1,-6,-5]
])

B = np.matrix([
  [ 0],
  [-2],
  [-7],
  [ 6],
])

print(np.linalg.inv(A).dot(B))

```

```{=typst}
#pagebreak()
== Exercice 2
```

```{python}

A = np.matrix([
  [ 5, 6 ],
  [ 6, 7 ],
  [ 1, 1 ]
])

B = np.matrix([
  [ 3],
  [ 1],
  [-5],
])


def f(A,B,x,y):
  ax = np.matrix([[x],[y]])
  return np.linalg.norm( A.dot(ax) - B )

```
```{=typst}

Ce syst√®me ne poss√®de pas de solution car il est surdetermin√© ( et il n'y pas de lignes similaires ).

$
mat(
  5,6;
  6,7;
  1,1;  
)
times 
mat(
  x;
  y; 
)
-
mat(
  3;
  1;
  5;  
)
$

on a donc

$
cases(
5 x + 6 y - 3 = 0,
6 x + 7 y - 1 = 0,
1 x + 1 y - 5 = 0,
)
$

pour calculer l'erreur, on doit calculer :

$ e = sqrt( (5 x + 6 y - 3)^2 + (6 x + 7 y - 1)^2 + (1 x + 1 y - 5)^2 ) $

apr√®s avoir d√©velopp√© tous les termes on trouve

$ e = sqrt( 62 x^2 + 146 x y - 32 x - 40 y + 86 y^2 + 44 ) $

et on fait quoi apr√®s ? Et bien, on cherche √† minimiser cette erreur, ce qui revient √† d√©terminer quand est-ce que la fonction ci-dessus a ses d√©riv√©s qui s'annulent

$ cases(
(diff e)/(diff x) = 124 x + 146 y - 32,
(diff e)/(diff y) = 146 x + 172 y - 40,
) $

On doit trouver ces deux diff√©rentielles √©gale √† 0

$ cases(
0 = 124 x + 146 y - 32,
0 = 146 x + 172 y - 40,
) $

$ cases(
146 y = 32 - 124 x,
172 y = 40 - 146 x,
) $

$ cases(
y = (32 - 124 x) / 146,
y = (40 - 146 x) / 172,
) $

$ (32 - 124 x) / 146 = (40 - 146 x) / 172 $
$ (32 - 124 x) times 172 = (40 - 146 x) times 146 $
$ 5504 - 21328 x = 5840 - 21316 x $
$ 336 = - 12 x $
$ x = - 28 $

$ y = (40 - 146 times (-28)) / 172 = (40 + 4088) / 172 = (4128) / 172 = 24 $

donc la solution qui minimise l'erreur est $x=vec(-28,24)$
```

```{python}
f(A,B,-28,24)
```

```{=typst}
#pagebreak()

=== Moore-Penrose pseudo-inverse
Maintenant on va v√©rifier si le r√©sultat pr√©c√©dent est bon avec la Moore-Penrose pseudo-inverse.
```
```{python}
import time

def solveMoore(A,B):
  A_plus = np.linalg.inv(A.T.dot(A)).dot(A.T)
  return A_plus.dot(B)

t = time.time()

res = solveMoore(A,B)

print("temps" , time.time()-t)
print(res)
```

On trouve la m√™me chose, mais le calcul est tellement simple qu'il est instantan√©.


```{=typst} 

=== passage √† l'√©chelle

Maintenant on va pousser les limites de cette m√©thode

```

```{python}

import numpy.random as rd

def surdeter(row,col):
  A = np.random.randint(1024, size=(row, col)) - 512 
  B = np.random.randint(1024, size=(row, 1)) - 512 
  return A,B

A,B = surdeter(40000,200)

t = time.time()

res = solveMoore(A,B)

print("temps" , time.time()-t)

```

```{=typst} 
M√™me si mon ordinateur est puissant, je ne vais pas allez plus loin que 40000 lignes et 200 colonnes. 3 secondes pour une matrice de cette taille c'est pl√ªtot rapide.

#pagebreak()

=== Regression polynomiale

Dans cette nouvelle partie, on veut approximer un nuage de points par un polynome du second degr√©.
```
```{python}

import matplotlib.pyplot as plt

pts = [(-1,1),(3,0),(0,1),(-2,-2),(2,3)]

for x,y in pts:
  plt.scatter(x, y)

xs = np.linspace(-2,3,100)

A = np.matrix([[pt[0]**i for i in range(3)] for pt in pts])
B = np.matrix([[pt[1]] for pt in pts])

res = solveMoore(A,B)

res2 = [r[0,0] for r in res]
res2.reverse()

plt.plot( xs,np.polyval(np.poly1d(res2),xs) )

plt.show()


RSS = 0
for pt in pts:
  RSS += ( pt[1] - np.poly1d(pt[0]) ) ** 2

print( np.sqrt( RSS / 6 )[0] )
print( np.linalg.norm(A.dot(res)-B) )

```
```{=typst} 

D'apr√®s ce que j'en d√©duis :

erreur $= sqrt((R S S)/(N+1))$

#pagebreak()

= Partie 3 :  Regession lineaire, descente de gradient

== Generate data
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Number of data points
N = 100

X = np.random.uniform(low=0, high=100, size=N)

# Making y = 2x + 1 + some gaussian or normal noise (assumption of linear regression itself)
Y = 2 * X + 1 + np.random.normal(scale=10, size=N)

# Plotting the data to see if it looks linear
plt.scatter(X, Y, edgecolors='black', color="red")
plt.show()
```
```{=typst}
les param√®tres d'un mod√®le lin√©aire sont la pente (w0) et l'ordonn√© √† l'origine (w1).

#pagebreak()
== Loss function
```
```{python}

def SSR(w,x,y):
    N = len(y)
    s = 0

    lw = len(w)
    fw = lambda x : sum( w[i]*x**(lw-i-1) for i in range(lw) )

    for i in range(0,N):
        s += ( fw( x[i] ) - y[i] )**2
    return s

def MSE(w,x,y):
    N = len(y)
    return SSR(w,x,y)/N

```

```{python}

print(SSR([2,1],X,Y))
print(MSE([2,1],X,Y))

```
```{=typst}
#pagebreak()
== Gradient descent
```
```{python}

def gradient(w,x,y):
    lw = len(w)
    fw = lambda x : sum( w[i]*x**(lw-i-1) for i in range(lw) )

    l = ( fw(x) - y )

    grad_w = np.array([ sum(2 * x ** (lw-i-1) * l) for i in range(lw) ])

    return grad_w

gradient([2,1],X,Y)
```

```{python}

def descenteGradient(w,x,y,¬µ,n):
    for _ in range(n):
        grad_w = gradient(w,x,y)
        w -= ¬µ * grad_w
    return w

def descenteGradient2(w,x,y,¬µ,œµ):
    normeE = np.inf
    while normeE > œµ:
        grad_w = gradient(w,x,y)
        w -= ¬µ * grad_w
        normeE = np.abs(np.max(grad_w))
    return w

print( descenteGradient([0,0],X,Y,10**(-6),100) )
print( descenteGradient2([0,0],X,Y,10**(-6),10**(-1)) )
```

```{=typst}
#pagebreak()
== Experiment
```

```{python}

def descenteGradient3(w,x,y,¬µ,n):
    l = []
    for _ in range(n):
        grad_w = gradient(w,x,y)
        l.append(grad_w)
        w -= ¬µ * grad_w
    l.append(w)
    return l

n=100
plt.plot(np.arange(0,n+1,1),[MSE( w, X, Y ) for w in descenteGradient3([0,0],X,Y,10**(-6),n)])

plt.show()
```

```{=typst}
√ßa converge vite.

Maintenant modifions un peu les param√®tres.
Enlevons l'al√©atoire, diminuons la tol√©rance de l'epsilon et augmentons le pas (¬µ).
```

```{python}

X = np.random.uniform(low=0, high=100, size=N)
Y = 2 * X + 1 # + np.random.normal(scale=10, size=N)

print( descenteGradient([0,0],X,Y,10**(-3),100) )
print( descenteGradient2([0,0],X,Y,10**(-3),10**(-1)) )
```

```{=typst}
Bizarre, on ne converge pas. C'est totalement normal, car comme on a augment√© ¬µ, d√®s que l'on va vouloir r√©calcul√© le gradient, celui-ci n'aura pas un coefficient trop grand pour descendre et va √† chaque it√©ration remonter.
```
```{python}

N = 100000


X = np.random.uniform(low=0, high=100, size=N)
Y = 2 * X + 1 + np.random.normal(scale=10, size=N)

plt.scatter(X, Y, edgecolors='black', color="red")
plt.show()

print( descenteGradient([0,0],X,Y,10**(-10),100) )
```

```{=typst}
Avec un ensemble de 10000 √©lements, j'ai du diminu√© ¬µ √† $10^(-10)$ pour converger.
```
