---
title: "Compte-rendu TP sur l'intégration numérique"
format: 
  typst:
    toc: true
    section-numbering: "1.1 :"
    author: Tom Chauvel
    keep-typ: true

render-on-save: true

engine: jupyter
jupyter: python3
---

= Generate data

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Number of data points
N = 100

X = np.random.uniform(low=0, high=100, size=N)

# Making y = 2x + 1 + some gaussian or normal noise (assumption of linear regression itself)
Y = 2 * X + 1 + np.random.normal(scale=10, size=N)

# Plotting the data to see if it looks linear
plt.scatter(X, Y, edgecolors='black', color="red")
plt.show()
```

les paramètres d'un modèle linéaire sont la pente (w0) et l'ordonné à l'origine (w1).

= Loss function

```{python}

def SSR(w,x,y):
    N = len(y)
    s = 0

    lw = len(w)
    fw = lambda x : sum( w[i]*x**(lw-i-1) for i in range(lw) )

    for i in range(0,N):
        s += ( fw( x[i] ) - y[i] )**2
    return s

def MSE(w,x,y):
    N = len(y)
    return SSR(w,x,y)/N

```

```{python}

print(SSR([2,1],X,Y))
print(MSE([2,1],X,Y))

```

= Gradient descent

```{python}

def gradient(w,x,y):
    lw = len(w)
    fw = lambda x : sum( w[i]*x**(lw-i-1) for i in range(lw) )

    l = ( fw(x) - y )

    grad_w = np.array([ sum(2 * x ** (lw-i-1) * l) for i in range(lw) ])

    return grad_w

gradient([2,1],X,Y)
```

```{python}

def descenteGradient(w,x,y,µ,n):
    for _ in range(n):
        grad_w = gradient(w,x,y)
        w -= µ * grad_w
    return w

def descenteGradient2(w,x,y,µ,ϵ):
    normeE = np.inf
    while normeE > ϵ:
        grad_w = gradient(w,x,y)
        w -= µ * grad_w
        normeE = np.abs(np.max(grad_w))
    return w

print( descenteGradient([0,0],X,Y,10**(-6),100) )
print( descenteGradient2([0,0],X,Y,10**(-6),10**(-1)) )
```

= Experiment

```{python}

X = np.random.uniform(low=0, high=100, size=N)
Y = 2 * X + 1 # + np.random.normal(scale=10, size=N)

print( descenteGradient([0,0],X,Y,10**(-3),100) )
print( descenteGradient2([0,0],X,Y,10**(-3),10**(-1)) )


```